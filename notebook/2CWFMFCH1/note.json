{
  "paragraphs": [
    {
      "title": "1. Load Data",
      "text": "import net.sansa_stack.rdf.spark.io.NTripleReader\nimport net.sansa_stack.rdf.spark.model.JenaSparkGraphXOps\nimport org.apache.spark.sql.SparkSession\nimport java.net.{URI \u003d\u003e JavaURI}\n\nimport net.sansa_stack.rdf.spark.graph.LoadGraph\nimport org.apache.spark.graphx.Graph\n\nval input \u003d \"hdfs://namenode:8020/data/GermanyPopulatedPlaces.nt\"\nval graph \u003d NTripleReader.load(spark, JavaURI.create(input))",
      "user": "anonymous",
      "dateUpdated": "Dec 27, 2017 2:31:52 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala",
        "title": true,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nimport net.sansa_stack.rdf.spark.io.NTripleReader\n\nimport net.sansa_stack.rdf.spark.model.JenaSparkGraphXOps\n\nimport org.apache.spark.sql.SparkSession\n\nimport java.net.{URI\u003d\u003eJavaURI}\n\nimport net.sansa_stack.rdf.spark.graph.LoadGraph\n\nimport org.apache.spark.graphx.Graph\n\ninput: String \u003d hdfs://namenode:8020/data/GermanyPopulatedPlaces.nt\n\ngraph: org.apache.spark.rdd.RDD[org.apache.jena.graph.Triple] \u003d MapPartitionsRDD[95] at map at NTripleReader.scala:39\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1509621876108_-341284568",
      "id": "20171102-112436_172970529",
      "dateCreated": "Nov 2, 2017 11:24:36 AM",
      "dateStarted": "Dec 27, 2017 2:31:52 PM",
      "dateFinished": "Dec 27, 2017 2:31:56 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "2. Compute statistics (i.e. Property Distribution)",
      "text": "import net.sansa_stack.rdf.spark.stats._\n\nval propertyDist \u003d PropertyUsage(graph, spark).PostProc()\n                   .map(f \u003d\u003e f._1.getLocalName+ \"\\t\" + f._2)\n\nprintln(\"%table Property Distribution\\tFrequency\\n \" + propertyDist.mkString(\"\\n\"))",
      "user": "anonymous",
      "dateUpdated": "Dec 27, 2017 2:32:02 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {
          "1": {
            "graph": {
              "mode": "pieChart",
              "height": 300.0,
              "optionOpen": false
            },
            "helium": {}
          }
        },
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nimport net.sansa_stack.rdf.spark.stats._\n\npropertyDist: Array[String] \u003d Array(type\t9103, BFO_0000050\t7199, label\t7183, population\t7183, geo\t7183, seeAlso\t3787, nameOfficialEN\t69, creator\t1, created\t1, title\t1, rights\t1, description\t1, rights\t1, source\t1)\n"
          },
          {
            "type": "TABLE",
            "data": "Property Distribution\tFrequency\n type\t9103\nBFO_0000050\t7199\nlabel\t7183\npopulation\t7183\ngeo\t7183\nseeAlso\t3787\nnameOfficialEN\t69\ncreator\t1\ncreated\t1\ntitle\t1\nrights\t1\ndescription\t1\nrights\t1\nsource\t1\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1509621997083_-1710050944",
      "id": "20171102-112637_201407605",
      "dateCreated": "Nov 2, 2017 11:26:37 AM",
      "dateStarted": "Dec 27, 2017 2:32:02 PM",
      "dateFinished": "Dec 27, 2017 2:32:10 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "3. Infer new Knowledge",
      "text": "import net.sansa_stack.inference.spark.forwardchaining.triples.{ForwardRuleReasonerOWLHorst, ForwardRuleReasonerRDFS, TransitiveReasoner}\nimport net.sansa_stack.inference.spark.data.loader.RDFGraphLoader\nimport java.net.{ URI \u003d\u003e JavaURI }\n\nval O_graph \u003d RDFGraphLoader.loadFromDisk(spark, JavaURI.create(input), 4)\nval reasoner \u003d new ForwardRuleReasonerRDFS(spark.sparkContext)\nval inferredGraph \u003d reasoner.apply(graph)\ninferredGraph.cache()",
      "user": "anonymous",
      "dateUpdated": "Dec 27, 2017 2:31:42 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala",
        "title": true,
        "tableHide": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nimport net.sansa_stack.inference.spark.forwardchaining.triples.{ForwardRuleReasonerOWLHorst, ForwardRuleReasonerRDFS, TransitiveReasoner}\n\nimport net.sansa_stack.inference.spark.data.loader.RDFGraphLoader\n\nimport java.net.{URI\u003d\u003eJavaURI}\n\nO_graph: net.sansa_stack.inference.spark.data.model.RDFGraph \u003d RDFGraph(MapPartitionsRDD[54] at map at RDFGraphLoader.scala:45)\n\nreasoner: net.sansa_stack.inference.spark.forwardchaining.triples.ForwardRuleReasonerRDFS \u003d net.sansa_stack.inference.spark.forwardchaining.triples.ForwardRuleReasonerRDFS@787c5c05\n\ninferredGraph: net.sansa_stack.inference.spark.data.model.RDFGraph \u003d RDFGraph(MapPartitionsRDD[91] at distinct at ForwardRuleReasonerRDFS.scala:217)\n\nres2: net.sansa_stack.inference.spark.data.model.RDFGraph \u003d RDFGraph(MapPartitionsRDD[91] at distinct at ForwardRuleReasonerRDFS.scala:217)\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1509622028081_1197324639",
      "id": "20171102-112708_848247359",
      "dateCreated": "Nov 2, 2017 11:27:08 AM",
      "dateStarted": "Dec 27, 2017 2:31:42 PM",
      "dateFinished": "Dec 27, 2017 2:31:48 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "3.1 Compute statistics (i.e Class Distribution to the original graph)",
      "text": "val rdf_stats_class_dist \u003d Used_Classes(graph, spark).PostProc()\n                           .map(f \u003d\u003e f._1.toString.substring(f._1.toString.lastIndexOf(\"/\") + 1)+ \"\\t\" + f._2)\nprintln(\"%table Class Distribution\\tFrequency\\n \" + rdf_stats_class_dist.mkString(\"\\n\"))",
      "user": "anonymous",
      "dateUpdated": "Dec 27, 2017 2:32:19 PM",
      "config": {
        "colWidth": 6.0,
        "enabled": true,
        "results": {
          "1": {
            "graph": {
              "mode": "lineChart",
              "height": 300.0,
              "optionOpen": false
            },
            "helium": {}
          }
        },
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nrdf_stats_class_dist: Array[String] \u003d Array(core#PopulatedPlace\t7183, fabio#WikipediaEntry\t1903, core#StateOrProvince\t16, Dataset\t1)\n"
          },
          {
            "type": "TABLE",
            "data": "Class Distribution\tFrequency\n core#PopulatedPlace\t7183\nfabio#WikipediaEntry\t1903\ncore#StateOrProvince\t16\nDataset\t1\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1509622058566_569460406",
      "id": "20171102-112738_1999790335",
      "dateCreated": "Nov 2, 2017 11:27:38 AM",
      "dateStarted": "Dec 27, 2017 2:32:19 PM",
      "dateFinished": "Dec 27, 2017 2:32:26 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "3.2 Compute statistics (i.e size of the inferred graph)",
      "text": "val I_Graph \u003d \"inferred graph\" + \"\\t\" + inferredGraph.size\nprintln(\"%table graph\\t size\\n \" + I_Graph)",
      "user": "anonymous",
      "dateUpdated": "Dec 27, 2017 2:33:04 PM",
      "config": {
        "colWidth": 6.0,
        "enabled": true,
        "results": {
          "1": {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false
            },
            "helium": {}
          }
        },
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nI_Graph: String \u003d inferred graph\t52709\n"
          },
          {
            "type": "TABLE",
            "data": "graph\t size\n inferred graph\t52709\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1509622096514_561642835",
      "id": "20171102-112816_1291571260",
      "dateCreated": "Nov 2, 2017 11:28:16 AM",
      "dateStarted": "Dec 27, 2017 2:32:59 PM",
      "dateFinished": "Dec 27, 2017 2:33:00 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "4. Compute resource ranking and get top 50 POIs.",
      "text": "val graph_rep \u003d LoadGraph(graph)\n   // Create a subgraph based on the vertices connected by geoproperty.\nval geoSubgraph \u003d\n   graph_rep.subgraph(t \u003d\u003e t.attr \u003d\u003d\n       \"http://www.w3.org/2006/vcard/ns#geo\")\n\nval pagerank \u003d geoSubgraph.pageRank(0.00001).vertices\nval report \u003d pagerank.join(geoSubgraph.vertices)\n     .map({ case (k, (r, v)) \u003d\u003e (r, v, k) })\n     .sortBy(50 - _._1)\n\ncase class POI(geoloc:Long, rank:Double, lat: String, lon: String)\nval POIs \u003d report.map{f\u003d\u003e\n   val geoloc \u003d f._3\n   val rank   \u003d f._1\n   val geo \u003d f._2.toString.split(\"[:,]+\")\n   val lat \u003d geo(1)\n   val lon \u003d geo(2)\n\n   POI(geoloc,rank,lat,lon)\n\n}.take(50)\n\nz.angularBind(\"pois\", POIs) // this is what sends the data to the map\n\nval reportPOI \u003d report.map(f \u003d\u003e f._3 + \"\\t\" + f._2 + \"\\t\" + f._1)\nprintln(\"%table geoloc \\t geocord \\t rank \\n \" + reportPOI.take(50).mkString(\"\\n\"))",
      "user": "anonymous",
      "dateUpdated": "Dec 27, 2017 2:36:52 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "\ngraph_rep: org.apache.spark.graphx.Graph[org.apache.jena.graph.Node,org.apache.jena.graph.Node] \u003d org.apache.spark.graphx.impl.GraphImpl@4b6db14f\n\ngeoSubgraph: org.apache.spark.graphx.Graph[org.apache.jena.graph.Node,org.apache.jena.graph.Node] \u003d org.apache.spark.graphx.impl.GraphImpl@6f0bf246\n\npagerank: org.apache.spark.graphx.VertexRDD[Double] \u003d VertexRDDImpl[537] at RDD at VertexRDD.scala:57\n\nreport: org.apache.spark.rdd.RDD[(Double, org.apache.jena.graph.Node, org.apache.spark.graphx.VertexId)] \u003d MapPartitionsRDD[548] at sortBy at \u003cconsole\u003e:61\n\ndefined class POI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 187.0 failed 4 times, most recent failure: Lost task 0.3 in stage 187.0 (TID 505, 192.168.160.7, executor 0): java.lang.ArrayIndexOutOfBoundsException: 2\n\tat $anonfun$1.apply(\u003cconsole\u003e:68)\n\tat $anonfun$1.apply(\u003cconsole\u003e:63)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:393)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n  at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1354)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n  at org.apache.spark.rdd.RDD.take(RDD.scala:1327)\n  ... 50 elided\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 2\n  at $anonfun$1.apply(\u003cconsole\u003e:68)\n  at $anonfun$1.apply(\u003cconsole\u003e:63)\n  at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n  at scala.collection.Iterator$$anon$10.next(Iterator.scala:393)\n  at scala.collection.Iterator$class.foreach(Iterator.scala:893)\n  at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n  at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n  at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n  at scala.collection.AbstractIterator.to(Iterator.scala:1336)\n  at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n  at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\n  at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n  at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\n  at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354)\n  at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354)\n  at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)\n  at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n  at org.apache.spark.scheduler.Task.run(Task.scala:99)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n  ... 3 more\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1509622142083_-934214916",
      "id": "20171102-112902_1189673617",
      "dateCreated": "Nov 2, 2017 11:29:02 AM",
      "dateStarted": "Dec 27, 2017 2:36:52 PM",
      "dateFinished": "Dec 27, 2017 2:37:17 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "5.Show POI on the map",
      "text": "%angular\n\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://cdnjs.cloudflare.com/ajax/libs/leaflet/0.7.5/leaflet.css\" /\u003e\n\u003cdiv id\u003d\"map\" style\u003d\"height: 800px; width: 100%\"\u003e\u003c/div\u003e\n\n\u003cscript type\u003d\"text/javascript\"\u003e\nfunction initMap() {\n    var map \u003d L.map(\u0027map\u0027).setView([30.00, -30.00], 3);\n\n    L.tileLayer(\u0027http://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\u0027, {\n        attribution: \u0027Map data \u0026copy; \u003ca href\u003d\"http://openstreetmap.org\"\u003eOpenStreetMap\u003c/a\u003e contributors\u0027,\n        maxZoom: 12,\n        minZoom: 3\n    }).addTo(map);\n\n    var geoMarkers \u003d L.layerGroup().addTo(map);\n\n    var el \u003d angular.element($(\u0027#map\u0027).parent(\u0027.ng-scope\u0027));\n    angular.element(el).ready(function() {\n        window.locationWatcher \u003d el.scope().compiledScope.$watch(\u0027pois\u0027, function(newValue, oldValue) {\n            // geoMarkers.clearLayers(); -- if you want to only show new data clear the layer first\n            angular.forEach(newValue, function(poi) {\n                var marker \u003d L.marker([ poi.lat, poi.lon ])\n                  .bindPopup(poi.geoloc + \": \" + poi.rank)\n                  .addTo(geoMarkers);\n            });\n        })\n    });\n}\n\nif (window.locationWatcher) {\n    // clear existing watcher otherwise we\u0027ll have duplicates\n    window.locationWatcher();\n}\n\n// ensure we only load the script once, seems to cause issues otherwise\nif (window.L) {\n    initMap();\n} else {\n    console.log(\u0027Loading Leaflet library\u0027);\n    var sc \u003d document.createElement(\u0027script\u0027);\n    sc.type \u003d \u0027text/javascript\u0027;\n    sc.src \u003d \u0027https://cdnjs.cloudflare.com/ajax/libs/leaflet/0.7.5/leaflet.js\u0027;\n    sc.onload \u003d initMap;\n    sc.onerror \u003d function(err) { alert(err); }\n    document.getElementsByTagName(\u0027head\u0027)[0].appendChild(sc);\n}\n\u003c/script\u003e\n",
      "user": "anonymous",
      "dateUpdated": "Nov 2, 2017 11:31:47 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/undefined",
        "editorHide": true,
        "tableHide": false,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "ANGULAR",
            "data": "\u003clink rel\u003d\"stylesheet\" href\u003d\"https://cdnjs.cloudflare.com/ajax/libs/leaflet/0.7.5/leaflet.css\" /\u003e\n\u003cdiv id\u003d\"map\" style\u003d\"height: 800px; width: 100%\"\u003e\u003c/div\u003e\n\n\u003cscript type\u003d\"text/javascript\"\u003e\nfunction initMap() {\n    var map \u003d L.map(\u0027map\u0027).setView([30.00, -30.00], 3);\n\n    L.tileLayer(\u0027http://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\u0027, {\n        attribution: \u0027Map data \u0026copy; \u003ca href\u003d\"http://openstreetmap.org\"\u003eOpenStreetMap\u003c/a\u003e contributors\u0027,\n        maxZoom: 12,\n        minZoom: 3\n    }).addTo(map);\n\n    var geoMarkers \u003d L.layerGroup().addTo(map);\n\n    var el \u003d angular.element($(\u0027#map\u0027).parent(\u0027.ng-scope\u0027));\n    angular.element(el).ready(function() {\n        window.locationWatcher \u003d el.scope().compiledScope.$watch(\u0027pois\u0027, function(newValue, oldValue) {\n            // geoMarkers.clearLayers(); -- if you want to only show new data clear the layer first\n            angular.forEach(newValue, function(poi) {\n                var marker \u003d L.marker([ poi.lat, poi.lon ])\n                  .bindPopup(poi.geoloc + \": \" + poi.rank)\n                  .addTo(geoMarkers);\n            });\n        })\n    });\n}\n\nif (window.locationWatcher) {\n    // clear existing watcher otherwise we\u0027ll have duplicates\n    window.locationWatcher();\n}\n\n// ensure we only load the script once, seems to cause issues otherwise\nif (window.L) {\n    initMap();\n} else {\n    console.log(\u0027Loading Leaflet library\u0027);\n    var sc \u003d document.createElement(\u0027script\u0027);\n    sc.type \u003d \u0027text/javascript\u0027;\n    sc.src \u003d \u0027https://cdnjs.cloudflare.com/ajax/libs/leaflet/0.7.5/leaflet.js\u0027;\n    sc.onload \u003d initMap;\n    sc.onerror \u003d function(err) { alert(err); }\n    document.getElementsByTagName(\u0027head\u0027)[0].appendChild(sc);\n}\n\u003c/script\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1509622213641_-1187071564",
      "id": "20171102-113013_1400142385",
      "dateCreated": "Nov 2, 2017 11:30:13 AM",
      "dateStarted": "Nov 2, 2017 11:30:50 AM",
      "dateFinished": "Nov 2, 2017 11:30:51 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Point of Interest Example",
  "id": "2CWFMFCH1",
  "angularObjects": {
    "2D4ERZXXW:shared_process": [],
    "2D4EPAJD4:shared_process": [],
    "2D3BKJA7V:shared_process": [],
    "2D22Q8VER:shared_process": [],
    "2D4BARTEP:shared_process": [],
    "2D4VXHXBJ:shared_process": [],
    "2D1HR4PUY:shared_process": [],
    "2D28CYVNT:shared_process": [],
    "2D3KBK32J:shared_process": [],
    "2D376ADYX:shared_process": [],
    "2D4ZA8GES:shared_process": [],
    "2D5974FPG:shared_process": [],
    "2D1RKBFHH:shared_process": [],
    "2D4J5PAAQ:shared_process": [],
    "2D55HQ7RC:shared_process": [],
    "2D3H8E9AU:shared_process": [],
    "2D271H4X8:shared_process": [],
    "2D37JTAJZ:shared_process": [],
    "2D4M2X1ES:shared_process": []
  },
  "config": {},
  "info": {}
}